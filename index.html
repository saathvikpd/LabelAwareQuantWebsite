<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>When Less is More: Surprising Gains from Label-Aware Quantization</title>
  <style>
    /* Global Styles */
    body {
      font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
      margin: 0;
      background-color: #f8f9fa;
      color: #333;
      line-height: 1.6;
    }
    header {
      background: linear-gradient(135deg, #4CAF50, #2E7D32);
      color: #fff;
      padding: 20px;
      text-align: center;
    }
    nav {
      background-color: #1976D2;
      display: flex;
      justify-content: center;
      padding: 10px 0;
    }
    nav a {
      color: #fff;
      text-decoration: none;
      padding: 10px 20px;
      margin: 0 10px;
      border-radius: 4px;
      transition: background 0.3s;
    }
    nav a:hover {
      background: rgba(255, 255, 255, 0.2);
    }
    .container {
      width: 90%;
      max-width: 1100px;
      margin: 20px auto;
      padding: 20px;
      background-color: #fff;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
      border-radius: 8px;
    }
    section {
      margin-bottom: 40px;
    }
    section h2 {
      margin-top: 0;
      color: #1976D2;
      border-bottom: 3px solid #1976D2;
      padding-bottom: 5px;
    }
    footer {
      background: #333;
      color: #fff;
      text-align: center;
      padding: 15px 10px;
    }

    /* Visualization Grid */
    .visualization-grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
      gap: 20px;
    }
    .visualization-item {
      background: #fff;
      border-radius: 8px;
      box-shadow: 0 4px 6px rgba(0,0,0,0.1);
      padding: 10px;
      text-align: center;
    }
    .visualization-item img {
      width: 100%;
      height: auto;
      max-height: 300px;
      object-fit: contain; /* preserves aspect ratio without stretching */
      border-radius: 4px;
      cursor: pointer;
      transition: transform 0.3s, border-color 0.3s;
    }
    .visualization-item img:hover {
      transform: scale(1.03);
      border-color: #1976D2;
    }

    /* Modal Styles */
    .modal {
      display: none;
      position: fixed;
      z-index: 1000;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      overflow: auto;
      background-color: rgba(0,0,0,0.8);
    }
    .modal-content {
      margin: 5% auto;
      display: block;
      max-width: 80%;
      max-height: 80vh;
      object-fit: contain; /* ensures no stretching */
      border-radius: 8px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .modal-caption {
      text-align: center;
      padding: 10px;
      color: #fff;
      font-size: 1.2em;
    }
    .close {
      position: absolute;
      top: 20px;
      right: 35px;
      color: #fff;
      font-size: 40px;
      font-weight: bold;
      cursor: pointer;
      transition: color 0.3s;
    }
    .close:hover {
      color: #bbb;
    }
  </style>
</head>
<body>
  <header>
    <h1>When Less is More: Surprising Gains from Label-Aware Quantization</h1>
    <p>Saathvik Dirisala, Jessica Hung, Ari Juljulian, Yijun Luo, Alex Cloninger, Rayan Saab<br>
       Halıcıoğlu Data Science Institute, UC San Diego</p>
  </header>

  <nav>
    <a href="#background">Background</a>
    <a href="#abstract">Abstract</a>
    <a href="#methods">Methods</a>
    <a href="#subset-generation">Subset Generation</a>
    <a href="#results">Results</a>
    <a href="#visualizations">Visualizations</a>
    <a href="#conclusions">Conclusions</a>
  </nav>

  <div class="container">
    <!-- Background Section -->
    <section id="background">
      <h2>Background</h2>
      <p>
        Deep convolutional neural networks have revolutionized computer vision tasks,
        achieving state-of-the-art performance on large-scale datasets. However, as models
        grow in complexity, deploying them on resource-constrained devices becomes challenging.
        Our research explores label-aware quantization to reduce model size while leveraging
        class-specific information to maintain or enhance performance.
      </p>
    </section>

    <!-- Abstract Section -->
    <section id="abstract">
      <h2>Abstract</h2>
      <p>
        In this project, we introduce a label-aware quantization strategy that compresses neural networks
        while enhancing performance, particularly for targeted subsets of data. By selectively quantizing
        weights and then fine-tuning on these subsets, we achieve surprising gains in accuracy and efficiency.
      </p>
    </section>

    <!-- Methods Section -->
    <section id="methods">
      <h2>Methods</h2>
      <ul>
        <li><strong>Quantization:</strong> A deterministic algorithm (GPFQ) maps full-precision weights to fewer bits.</li>
        <li><strong>Subset Generation:</strong> Classes are grouped based on similarity, allowing focused quantization.</li>
        <li><strong>Fine-Tuning:</strong> Post-quantization, models are fine-tuned to recover or improve accuracy.</li>
      </ul>
    </section>

    <!-- Subset Generation Section -->
    <section id="subset-generation">
      <h2>Subset Generation</h2>
      <p>
        We generate subsets of classes from CIFAR100 to explore how label-aware quantization performs under
        different levels of class diversity. Similar classes are grouped to minimize accuracy loss while maximizing
        compression benefits.
      </p>
    </section>

    <!-- Results Section -->
    <section id="results">
      <h2>Results</h2>
      <p>
        Our experiments reveal that quantized models can outperform or match the original model on specific class subsets,
        highlighting the effectiveness of label-aware approaches in reducing complexity without sacrificing accuracy.
      </p>
    </section>

    <!-- Visualizations Section -->
    <section id="visualizations">
      <h2>Visualizations</h2>
      <div class="visualization-grid">
        <div class="visualization-item">
          <img src="images/Quantization-Process.png"
               alt="Quantization Process Flowchart"
               data-caption="Quantization Process Flowchart">
        </div>
        <div class="visualization-item">
          <img src="images/resnet50.png"
               alt="ResNet50 Graph"
               data-caption="ResNet50 Performance Graph">
        </div>
        <div class="visualization-item">
          <img src="images/umap.png"
               alt="UMAP Visualization"
               data-caption="UMAP Visualization of Class Embeddings">
        </div>
      </div>
    </section>

    <!-- Conclusions Section -->
    <section id="conclusions">
      <h2>Conclusions</h2>
      <p>
        Label-aware quantization proves to be a viable strategy for model compression, preserving or improving
        accuracy on carefully chosen subsets of data. Future work will extend these techniques to more complex
        architectures and larger datasets.
      </p>
    </section>
  </div>

  <footer>
    <p>&copy; 2025 Label-Aware Quantization | Halıcıoğlu Data Science Institute, UC San Diego</p>
  </footer>

  <!-- Modal Structure -->
  <div id="modal" class="modal">
    <span class="close">&times;</s
